So in my project, I have 10 chair classes, and we will test one of them to check if the detection is accurate.


In this graph, it displays the specific accuracy percentage for each class.

There is also a pie chart that shows the detected distribution of the 10 classes. In this chart, the ball chair appears as 100% because it is the only class detected at that time.

Additionally, there is a scan trend section that records how many times a specific class has been scanned. Based on this data, the ball chair has been scanned three times.


Next, we try another class, which is the rocking chair. As usual, we start by using the camera. After capturing the image, we immediately see the result, and it is correct—the object is detected as a rocking chair with 100% confidence.

We then try uploading an image, and it is still accurately detected as a rocking chair with 100% confidence. After that, we go back to using the camera again to check if the accuracy is consistent. The result is still correct, showing a rocking chair with 100% confidence.

When we look at the graph, we can see that the rocking chair class has been added. In the pie chart, all four classes now have equal percentages. In the scan trend, the rocking chair also appears, showing that it has been scanned three times.

Now, for the chaise lounge chair, we try using image upload first to properly check the accuracy. On the first attempt, it is correctly detected as a chaise lounge chair with 100% confidence. We try again using a different image, and the result is still correct, showing a chaise lounge chair with a confidence of 99.95%. In the accuracy breakdown, we can also see where the remaining 0.05% is distributed.

We then upload another image, and once again, it is correctly detected as a chaise lounge chair with 100% confidence. In the logs, we can see that all the images we uploaded were detected accurately. The prediction analysis also explains why that specific class was detected.

In the analytics section, the chaise lounge chair has now been added, which indicates that our graphs and pie charts are functioning correctly. We can also see a 100% average confidence, which shows that there is a high chance that our chair detector app is successful and reliable.



We then try another class using the camera, which is the waiting chair. When using the camera, the image must be clear and properly captured to ensure accurate results. With a clear photo, the system correctly detects it as a waiting chair with 99.98% confidence.

When we upload an image, there is a higher chance of reaching 100% accuracy because the image is clearer. We then take another picture of the waiting chair with a very clear image, and this time the result reaches 100% confidence. After this, the graphs and pie charts are updated accordingly to reflect the new scan results.


We then try another class using only the camera. It is correctly detected as a hammock chair with 100% accuracy. We try using a different image to check if the result is still correct, and it is again detected as a hammock chair with 100% confidence. For the last capture, we take another image, and the result is still correct. This shows that the camera is fully working for class detection, since all three attempts produced 100% accuracy. The graph is also updated accordingly.

Next, we test another class using the camera, which is the plastic chair. It is correctly detected with an accuracy of 99.45%. In the prediction breakdown, we can see where the remaining 0.55% is distributed. In the graph, the plastic chair is added. In the pie chart, the percentage for the plastic chair is still small because it has only been scanned once, so it has a smaller share of the total. In the scan trend, we can also see that it has been scanned only once.

We then scan the plastic chair again using the same image, and this time it reaches 100% accuracy. The pie chart is updated from one scan to two scans, and its percentage increases. The scan trend also updates to show two scans. In the history, we can see that the same image produced different accuracy values, which depends on how the image was captured. This highlights a disadvantage of using the camera, since accuracy can vary based on lighting, angle, and image quality.

We try another capture using the camera but with a different and clearer image, and it again reaches 100% accuracy because the image clearly shows a plastic chair. The graph and pie chart in the analytics section are updated once more.

After testing with the camera, we now switch to image uploads from the gallery. The detected class is a wheelchair, which is correct, with a confidence of 99.98%. We try another image, and it is still correctly detected as a wheelchair with 99.94% confidence. For the last image we test, it is again correctly detected as a wheelchair with 99.90% confidence.

In the analytics section, the wheelchair class is added with a confidence of 99.98%, and both the pie chart and scan trend are updated accordingly.






10. The last class we tested in the app is the high chair. We scanned it using the camera to check if the detection is accurate. The result is correct—it was detected as a high chair with a confidence of 98.82%.

Next, we tried uploading an image to see if the app could still detect it correctly. The result was again accurate, detecting a high chair with 99.94% confidence. We then went back to using the camera, and this time the result reached 100% confidence because the image was very clear.

In the analytics section, the graph is updated to include the high chair. In the pie chart, all classes are now shown, and they have equal percentages. The same update can also be seen in the scan trend graph, indicating that all classes have been scanned equally.


QUESTION

I collected hundreds of images for each class and uploaded them to Teachable Machine. At first, it was difficult to achieve high accuracy, like 95%, especially if the images uploaded had backgrounds, because those backgrounds would also be included in the training. What I did was look for images without backgrounds, so only the specific class was visible. This allowed the model to be trained properly. This is also one of the main factors why my classes are detected so accurately—the images I uploaded are clear and background-free, making training more effective.


I chose on-device local model inference instead of cloud-based ML inference because I wanted the detection to be faster and work without relying on internet connectivity. With on-device inference, the app can process images immediately, giving real-time results, which is important for a camera-based detection system. The trade-offs are that on-device models are limited by the device’s memory and processing power, so very large models or extremely complex tasks may not run as efficiently. In contrast, cloud-based inference can handle bigger models and more complex computations, but it requires internet access, may have latency, and raises data privacy concerns since images are sent to external servers.


A real-time dashboard can greatly improve decision-making and monitoring in real-world applications of my project because it allows users to see the current status of all detected classes immediately. For example, we can track how many times each chair type has been scanned, the confidence levels, and trends over time. This instant visibility helps users quickly identify patterns, anomalies, or errors, so they can make informed decisions right away. In addition, it allows for better management of resources and planning, since all the data is presented in an organized and visual way, making it easier to interpret and act on.